{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nuranynovita/cohort-analysis-customer-retention?scriptVersionId=143746573\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Introduction\n\nCohort analysis is a type of behavioral analytics that can help you understand the health of your business and the loyalty of your customers. To build a cohort, customers are divided into cohorts or groups based on similarities to those groups and specific traits. The term cohort refers to a time-span grouping that divides people according to the week or month they were first acquired.\n\nThere are two ways to break the group of users into cohorts based on [Clevertap.com](https://clevertap.com/blog/cohort-analysis/):\n\n* **Acquisition Cohorts**: divide users by when they signed up first for your product. For your app users, you might break down your cohorts by the day, the week or the month they launched an app, and thereby track daily, weekly or monthly cohorts.\n\n\n* **Behavioral Cohorts**: divide users by the behaviors they have (or haven’t) taken in your app within a given time period. These could be any number of discrete actions that a user can perform – App Install, App Launch, App Uninstall, Transaction or Charged, or any combination of these actions / events.\n\nIn this project, we will divide customers into acquisition cohorts based on initial purchase from the online Retail II dataset. The dataset contains all sales transactions for UK-based and registered customers. The company mainly sells unique all-occasion gift-ware. We will investigate customer retention rate by monthly cohort with each cohort of customers grouped by initial purchase.","metadata":{}},{"cell_type":"markdown","source":"The dataset contains 1,067,371 rows and 8 columns.\n\n* InvoiceNo: Invoice number\n* StockCode: Product (item) code\n* Description: Product (item) name\n* Quantity: The quantities of each product (item) per transaction\n* InvoiceDate: Invoice date and time\n* UnitPrice: Unit price. Product price per unit in sterling (Â£).\n* CustomerID: Customer number\n* Country: Country name","metadata":{}},{"cell_type":"code","source":"# import libraries\n \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport seaborn as sns\nimport datetime as dt\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(\"ignore\")\npd.set_option('display.max_columns', 100)\nfrom pandas_profiling import ProfileReport \n%matplotlib inline\n\n# import data\ndf_retail = pd.read_csv(\"../input/online-retail-ii-uci/online_retail_II.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:40:11.80112Z","iopub.execute_input":"2023-01-06T12:40:11.80287Z","iopub.status.idle":"2023-01-06T12:40:18.966643Z","shell.execute_reply.started":"2023-01-06T12:40:11.802673Z","shell.execute_reply":"2023-01-06T12:40:18.964413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_retail.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:40:18.969848Z","iopub.execute_input":"2023-01-06T12:40:18.970379Z","iopub.status.idle":"2023-01-06T12:40:19.007322Z","shell.execute_reply.started":"2023-01-06T12:40:18.970337Z","shell.execute_reply":"2023-01-06T12:40:19.006285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_retail.info()","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:40:19.008713Z","iopub.execute_input":"2023-01-06T12:40:19.009309Z","iopub.status.idle":"2023-01-06T12:40:19.30445Z","shell.execute_reply.started":"2023-01-06T12:40:19.009273Z","shell.execute_reply":"2023-01-06T12:40:19.302769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploration Data","metadata":{}},{"cell_type":"code","source":"# Variables & Distribution\ndf_retail.profile_report()","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:40:19.306769Z","iopub.execute_input":"2023-01-06T12:40:19.308334Z","iopub.status.idle":"2023-01-06T12:40:53.283929Z","shell.execute_reply.started":"2023-01-06T12:40:19.308267Z","shell.execute_reply":"2023-01-06T12:40:53.282147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"According to the overview above, there are 247389 rows missing values and 25061 duplicate rows. 'Description' and 'CustomerID' have the most missing values. So, I'll remove some rows from 'CustomerID' that have a high percentage of missing values. \n\nWe will remove 25061 duplicate rows from the dataset. Several columns data type will be changed. And then, We will add a necessary column for further analysis, such as the 'Total Sales' column, which is calculated from the 'Quantity' and 'Price' calculations. \n\nIn case we have a dataset with a time span of 2010-2011. We assume that the first purchase occurred in January 2010. The dataset for the 2009 year is incomplete because it only includes data in December. So, we will exclude it.","metadata":{}},{"cell_type":"code","source":"# drop missing value in 'Customer ID' column\ndf_retail.dropna(subset=['Customer ID'],inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:40:53.290151Z","iopub.execute_input":"2023-01-06T12:40:53.290722Z","iopub.status.idle":"2023-01-06T12:40:53.403619Z","shell.execute_reply.started":"2023-01-06T12:40:53.290668Z","shell.execute_reply":"2023-01-06T12:40:53.401506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# change column data type\ndf_retail['InvoiceDate'] = pd.to_datetime(df_retail['InvoiceDate'])\ndf_retail.sort_values(by=['InvoiceDate'])\ndf_retail['Customer ID'] = df_retail['Customer ID'].astype(np.int64)\n\n# add 'Total' column\ndf_retail['Total Sales'] = df_retail['Quantity'] * df_retail['Price']\n\n#exclude data in 2009\ndf_retail = df_retail[df_retail['InvoiceDate'].dt.year != 2009]","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:40:53.40621Z","iopub.execute_input":"2023-01-06T12:40:53.407196Z","iopub.status.idle":"2023-01-06T12:40:53.993264Z","shell.execute_reply.started":"2023-01-06T12:40:53.407136Z","shell.execute_reply":"2023-01-06T12:40:53.991278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check duplicate rows\ndf_retail[df_retail.duplicated()]","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:40:53.995514Z","iopub.execute_input":"2023-01-06T12:40:53.995964Z","iopub.status.idle":"2023-01-06T12:40:54.806774Z","shell.execute_reply.started":"2023-01-06T12:40:53.995925Z","shell.execute_reply":"2023-01-06T12:40:54.805391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# drop duplicate rows\ndf_retail.drop_duplicates(inplace=True)\ndf_retail.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:40:54.808578Z","iopub.execute_input":"2023-01-06T12:40:54.80929Z","iopub.status.idle":"2023-01-06T12:40:55.618758Z","shell.execute_reply.started":"2023-01-06T12:40:54.809209Z","shell.execute_reply":"2023-01-06T12:40:55.617067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#check negative quantity\ndf_retail[df_retail['Quantity'] < 0]\n","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:40:55.621544Z","iopub.execute_input":"2023-01-06T12:40:55.62212Z","iopub.status.idle":"2023-01-06T12:40:55.669616Z","shell.execute_reply.started":"2023-01-06T12:40:55.622071Z","shell.execute_reply":"2023-01-06T12:40:55.667629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset have negative quantities which means some of orders are most return. We will remove it because we won't be necessary it for this analysis.","metadata":{}},{"cell_type":"code","source":"# drop negative quantity\ndf_retail.drop(df_retail[df_retail['Quantity'] < 0].index, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:40:55.671812Z","iopub.execute_input":"2023-01-06T12:40:55.67364Z","iopub.status.idle":"2023-01-06T12:40:55.79804Z","shell.execute_reply.started":"2023-01-06T12:40:55.673586Z","shell.execute_reply":"2023-01-06T12:40:55.796421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_retail.info()","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:40:55.80021Z","iopub.execute_input":"2023-01-06T12:40:55.800691Z","iopub.status.idle":"2023-01-06T12:40:55.952265Z","shell.execute_reply.started":"2023-01-06T12:40:55.800655Z","shell.execute_reply":"2023-01-06T12:40:55.951061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Number of Transaction","metadata":{}},{"cell_type":"code","source":"# extract hour, day, and month\ndf_retail['dayofweek'] = df_retail['InvoiceDate'].dt.strftime(\"%A\")\ndf_retail['hour'] = df_retail['InvoiceDate'].dt.strftime('%H')\ndf_retail['month'] = df_retail['InvoiceDate'].dt.strftime('%Y-%m')","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:40:55.953566Z","iopub.execute_input":"2023-01-06T12:40:55.953904Z","iopub.status.idle":"2023-01-06T12:41:08.247515Z","shell.execute_reply.started":"2023-01-06T12:40:55.953873Z","shell.execute_reply":"2023-01-06T12:41:08.245832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_retail['dayofweek'].unique()","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:41:08.249328Z","iopub.execute_input":"2023-01-06T12:41:08.249719Z","iopub.status.idle":"2023-01-06T12:41:08.325931Z","shell.execute_reply.started":"2023-01-06T12:41:08.249687Z","shell.execute_reply":"2023-01-06T12:41:08.323402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pivot number of transaction per hour of day and day of the week\ncategories = [ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Sunday']\ntr_count_pivot = df_retail.pivot_table(index='dayofweek', columns='hour', values='Invoice', aggfunc='nunique').fillna(0).loc[categories, (slice(None))]\nprint(tr_count_pivot)","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:41:08.332619Z","iopub.execute_input":"2023-01-06T12:41:08.333081Z","iopub.status.idle":"2023-01-06T12:41:08.876166Z","shell.execute_reply.started":"2023-01-06T12:41:08.333044Z","shell.execute_reply":"2023-01-06T12:41:08.874028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of transaction heatmap\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.heatmap(tr_count_pivot,\n            annot=True, \n            fmt='g', \n            cmap='YlGnBu',\n            vmin=0.0,\n            vmax=1200,\n            ax=ax)\nax.set_title('Number of Transaction By Per Hour of Day In Day of The Week', fontsize=16)\nax.set(xlabel='Hour of Day', ylabel='Day of The Week')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:41:08.878776Z","iopub.execute_input":"2023-01-06T12:41:08.879348Z","iopub.status.idle":"2023-01-06T12:41:10.185484Z","shell.execute_reply.started":"2023-01-06T12:41:08.879297Z","shell.execute_reply":"2023-01-06T12:41:10.183386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It's interesting because there is no transaction on Saturday. As we can see, the most transactions occurred between 10 am and 15 pm. The peak transactions occurred on Wednesday at 12 pm.","metadata":{}},{"cell_type":"code","source":"# count transaction by distinct customer ID\ncount_tr = df_retail.groupby(['month', 'Customer ID'])['Invoice'].nunique().reset_index()\nprint(count_tr)","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:41:10.187573Z","iopub.execute_input":"2023-01-06T12:41:10.189027Z","iopub.status.idle":"2023-01-06T12:41:10.414421Z","shell.execute_reply.started":"2023-01-06T12:41:10.188932Z","shell.execute_reply":"2023-01-06T12:41:10.412141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"month_tr = count_tr.groupby(['month'])['Invoice'].sum().reset_index()\nprint(month_tr)","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:41:10.416663Z","iopub.execute_input":"2023-01-06T12:41:10.417166Z","iopub.status.idle":"2023-01-06T12:41:10.436031Z","shell.execute_reply.started":"2023-01-06T12:41:10.417127Z","shell.execute_reply":"2023-01-06T12:41:10.433898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# monthly number of transaction lineplot\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.lineplot(data=month_tr,\n             x=month_tr['month'],\n             y=month_tr['Invoice'],\n             marker='o', \n             linestyle='-')\nax.set_title('Monthly Number of Transaction', fontsize=16)\nax.set(xlabel='Month', ylabel='Transaction')\n# label points on the lineplot\nfor x, y in zip(month_tr['month'], month_tr['Invoice']):\n    plt.text(x = x,\n             y = y+45,\n             s = '{:.0f}'.format(y),\n             color = 'red')\n    # add set_backgroundcolor(‘color’) after plt.text(‘…’)\n    plt.text(x, y+45, '{:.0f}'.format(y), color='white').set_backgroundcolor('#965786')\nfig.autofmt_xdate()\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:41:10.438183Z","iopub.execute_input":"2023-01-06T12:41:10.438699Z","iopub.status.idle":"2023-01-06T12:41:11.434834Z","shell.execute_reply.started":"2023-01-06T12:41:10.438661Z","shell.execute_reply":"2023-01-06T12:41:11.433325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We notice that the curve has a similar pattern. It consistently peaks in March, June, September, and October. The highest monthly number of transactions occurred in November, followed by a fall in December. Perhaps the increase in transactions is related to the Holiday Season.","metadata":{}},{"cell_type":"markdown","source":"## Monthly Active Users","metadata":{}},{"cell_type":"code","source":"# assign monthly active user\nmau = df_retail.groupby(['month'])['Customer ID'].nunique().reset_index()\nprint(mau)","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:41:11.4367Z","iopub.execute_input":"2023-01-06T12:41:11.437869Z","iopub.status.idle":"2023-01-06T12:41:11.586567Z","shell.execute_reply.started":"2023-01-06T12:41:11.437786Z","shell.execute_reply":"2023-01-06T12:41:11.585035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# monthly customer active lineplot\nfig, ax = plt.subplots(figsize=(12, 8))\nsns.lineplot(data=mau,\n             x=mau['month'],\n             y=mau['Customer ID'],\n             marker='o', \n             linestyle='-')\nax.set_title('Monthly Customer Active', fontsize=16)\nax.set(xlabel='Month', ylabel='Customer')\n# label points on the lineplot\nfor x, y in zip(mau['month'], mau['Customer ID']):\n    plt.text(x = x,\n             y = y,\n             s = '{:.0f}'.format(y),\n             color = 'red')\n    # add set_backgroundcolor(‘color’) after plt.text(‘…’)\n    plt.text(x, y, '{:.0f}'.format(y), color='white').set_backgroundcolor('#965786')\nfig.autofmt_xdate()\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:41:11.587855Z","iopub.execute_input":"2023-01-06T12:41:11.588198Z","iopub.status.idle":"2023-01-06T12:41:12.520083Z","shell.execute_reply.started":"2023-01-06T12:41:11.588167Z","shell.execute_reply":"2023-01-06T12:41:12.518445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above curve pattern is the same as the number of transaction curve. It is caused by customers being more active in making purchases throughout the Holiday Season. The highest monthly active users were in November 2010 and November 2011.","metadata":{}},{"cell_type":"markdown","source":"## Cohort Analysis\n\nWe will build cohort analysis based on what I've learned on [here](https://campus.datacamp.com/courses/customer-segmentation-in-python/). First, we will assign acquisition month cohort each customer in order to create a cohort month. Next, we will calculate the time offset by extracting integer values of the date for cohort index. Then, we will calculate the number of monthly active customers in each cohort. The cohort table is being plotted in a heatmap.\n\n","metadata":{}},{"cell_type":"code","source":"# assign acquisition month cohort\ndef get_month(x): \n    return dt.datetime(x.year, x.month, 1)\n\ndf_retail = df_retail[df_retail['InvoiceDate'].dt.year != 2009]\ndf_retail['InvoiceMonth'] = df_retail['InvoiceDate'].apply(get_month)\ngroup = df_retail.groupby('Customer ID')['InvoiceMonth']\ndf_retail['CohortMonth'] = group.transform('min').dt.to_period('M')\ndf_retail.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:41:12.521909Z","iopub.execute_input":"2023-01-06T12:41:12.523174Z","iopub.status.idle":"2023-01-06T12:41:17.197329Z","shell.execute_reply.started":"2023-01-06T12:41:12.523096Z","shell.execute_reply":"2023-01-06T12:41:17.195651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# assign time offset value\ndef get_date_int(df_retail, column):\n    year = df_retail[column].dt.year\n    month = df_retail[column].dt.month\n    day = df_retail[column].dt.day\n    return year, month, day\n\n# Getting the integers for date parts from the `InvoiceMonth` column\ntranscation_year, transaction_month, _ = get_date_int(df_retail, 'InvoiceMonth')\n# Getting the integers for date parts from the `CohortMonth` column\ncohort_year, cohort_month, _ = get_date_int(df_retail, 'CohortMonth')\n#  Get the  difference in years\nyears_diff = transcation_year - cohort_year\n# Calculate difference in months\nmonths_diff = transaction_month - cohort_month\n\"\"\" Extract the difference in months from all previous values\n \"+1\" in addeded at the end so that first month is marked as 1 instead of 0 for easier interpretation. \n \"\"\"\ndf_retail['CohortIndex'] = years_diff * 12 + months_diff  + 1 \ndf_retail.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:41:17.198851Z","iopub.execute_input":"2023-01-06T12:41:17.199502Z","iopub.status.idle":"2023-01-06T12:41:17.832177Z","shell.execute_reply.started":"2023-01-06T12:41:17.199464Z","shell.execute_reply":"2023-01-06T12:41:17.82987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Counting monthly active user from each chort\ngrouping = df_retail.groupby(['CohortMonth', 'CohortIndex'])\n# Counting number of unique customer Id's falling in each group of CohortMonth and CohortIndex\ncohort_data = grouping['Customer ID'].apply(pd.Series.nunique)\ncohort_data = cohort_data.reset_index()\n# Assigning column names to the dataframe created above\ncohort_counts = cohort_data.pivot(index='CohortMonth',\n                                 columns ='CohortIndex',\n                                 values = 'Customer ID')\n# Print top 5 rows of Dataframe\ncohort_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:41:17.834419Z","iopub.execute_input":"2023-01-06T12:41:17.834957Z","iopub.status.idle":"2023-01-06T12:41:17.998946Z","shell.execute_reply.started":"2023-01-06T12:41:17.834907Z","shell.execute_reply":"2023-01-06T12:41:17.997253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Coverting the retention rate into percentage and rounding off.\ncohort_sizes = cohort_counts.iloc[:,0]\nretention = cohort_counts.divide(cohort_sizes, axis=0)\nretention_rate = retention.round(3)*100","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:41:18.000448Z","iopub.execute_input":"2023-01-06T12:41:18.001206Z","iopub.status.idle":"2023-01-06T12:41:18.016633Z","shell.execute_reply.started":"2023-01-06T12:41:18.001168Z","shell.execute_reply":"2023-01-06T12:41:18.015079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with sns.axes_style(\"white\"):\n    fig, ax = plt.subplots(1, 2, figsize=(15, 10), sharey=True, gridspec_kw={'width_ratios': [1, 11]})\n    \n    # retention matrix\n    sns.heatmap(retention_rate, \n                mask=retention_rate.isnull(), \n                annot=True, \n                fmt='g', \n                cmap='RdYlGn',\n                vmin=0.0, \n                vmax=50,\n                ax=ax[1])\n    ax[1].set_title('Customer Retention Rate by Monthly Cohorts', fontsize=16)\n    ax[1].set(xlabel='Cohort Index',\n              ylabel='')\n\n    # cohort size\n    cohort_size_df = pd.DataFrame(cohort_sizes).rename(columns={1: 'Cohort Size'})\n    white_cmap = mcolors.ListedColormap(['white'])\n    sns.heatmap(cohort_size_df, \n                annot=True, \n                cbar=False, \n                fmt='g', \n                cmap=white_cmap, \n                ax=ax[0])\n\n    fig.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-06T12:41:18.018468Z","iopub.execute_input":"2023-01-06T12:41:18.01888Z","iopub.status.idle":"2023-01-06T12:41:21.065408Z","shell.execute_reply.started":"2023-01-06T12:41:18.018841Z","shell.execute_reply":"2023-01-06T12:41:21.06378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\n\nBased on our findings, we will summarize insights that we've learned:\n\n* The most transactions occurred between 10 am and 15 pm. The peak transactions occurred on Wednesday at 12 pm.\n* The highest monthly number of transactions occurred in November due to the Holiday Season.\n* The highest monthly active users were in November 2010 and November 2011.\n* As we can see, the customers from January and February 2010 have a better retention rate overall, and the longer the customers have been with the online retail.\n* 47.2% of cohorts that purchased in January 2010 were still active 3 months later. This suggests that the online retail is successful in retaining 47.2% of its customers.\n* Customers who remained active for 3 months were retained. To build customer retention strategies, we can create a remarketing campaign such as free delivery, coupons, loyalty rewards, and so on. So, after 3 months, we can keep customers coming back for repeat purchases.","metadata":{}}]}